data("Affairs")
install.packages("AER")
library(AER)
data("Affairs")
data("Affairs")
head(Affairs, 20)
summary(Affairs)
Affairs$affairs <- ifelse(Affairs$affairs >= 1 , "Yes" , "No")
Affairs$affairs <- ifelse(Affairs$affairs >= 1 , "Yes" , "No")
table(Affairs$affairs)
Affairs$affairs <- if else(Affairs$affairs >= 1 , "Yes" , "No")
table(Affairs$affairs)
Affairs$affairs
data("Affairs")
#viewing and analysing the data
head(Affairs, 20)
sum(is.na(Affairs))
summary(Affairs)
#TASK 1
#Create a new column with nominal values "YES" and "NO"
Affairs$affairs
Affairs$affairs <- ifelse(Affairs$affairs > 0 , "Yes" , "No")
table(Affairs$affairs)
Affairs$affairs <- as.factor(Affairs$affairs)
class(Affairs$affairs)
set.seed(100)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set <- Affairs[forTrainig == 1,]
test_set <- Affairs[forTrainig ==2, ]
nrow(train_set)
nrow(test_set)
class(train_set)
install.packages("rpart.plot")
install.packages("rpart")
library(rpart)
library(rpart.plot)
fit <- rpart(train_set$affairs ~., data = train_set)
fit <- rpart(train_set$affairs ~., data = train_set)
summary(fit)
predict_model <- predict(fit, newdata = test_set, type = "class")
summary(predict_model)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
confusionMatrix(table(predict_model, test_set$affairs))
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set <- Affairs[forTrainig == 1,]
test_set <- Affairs[forTrainig ==2, ]
nrow(train_set)
nrow(test_set)
class(train_set)
fit_random <- randomForest(train_set$affairs, data = train_set)
library(randomForest)
fit_random <- randomForest(train_set$affairs, data = train_set)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set <- Affairs[forTrainig == 1,]
test_set <- Affairs[forTrainig ==2, ]
nrow(train_set)
nrow(test_set)
class(train_set)
fit_random <- randomForest(train_set$affairs, data = train_set)
fit_random <- randomForest(train_set$affairs, data = train_set)
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set <- Affairs[forTrainig == 1,]
test_set <- Affairs[forTrainig ==2, ]
nrow(train_set)
nrow(test_set)
class(train_set)
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set_random <- Affairs[forTrainig == 1,]
test_set_random <- Affairs[forTrainig ==2, ]
nrow(train_set_random)
nrow(test_set_random)
class(train_set_random)
fit_random <- randomForest(affairs~. , data = train_set_random)
summary(fit_random)
predic_random <- predict(fit_random, data = test_set_random)
summary(predic_random)
predic_random <- predict(fit_random, data = test_set_random)
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set_random <- Affairs[forTrainig == 1,]
test_set_random <- Affairs[forTrainig ==2, ]
nrow(train_set_random)
nrow(test_set_random)
class(train_set_random)
fit_random <- randomForest(affairs~. , data = train_set_random)
summary(fit_random)
predic_random <- predict(fit_random, data = test_set_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
predict_model <- predict(fit, newdata = test_set, type = "class")
summary(predict_model)
confusionMatrix(table(predict_model, test_set$affairs))
fit_random <- randomForest(affairs~. , data = train_set_random)
summary(fit_random)
fit_random <- randomForest(affairs~. , data = train_set_random)
summary(fit_random)
predic_random <- predict(fit_random, newdata = data = test_set_random. type = "class")
predic_random <- predict(fit_random, newdata = test_set_random. type = "class")
predic_random <- predict(fit_random, newdata = test_set_random, type = "class")
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
importance(test_set_random$affairs)
importance(affairs)
?importance
importance(test_set_random)
importance(train_set_random$affairs)
prp(tree)
prp(Affairs)
rpart.plot(fit, box.palette="RdBu", shadow.col="gray", nn=TRUE)
rpart.plot(fit_random, box.palette="RdBu", shadow.col="gray", nn=TRUE)
fit_random <- randomForest(affairs~. , data = train_set_random, ntree = 200)
summary(fit_random)
predic_random <- predict(fit_random, newdata = test_set_random, type = "class")
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
oobdata = as.data.table(plot(fit_random))
#GET OBB data from the plot and coerce to data.table
library(data.table)
oobdata = as.data.table(plot(fit_random))
#Define trees as 1:ntree
oobdata[,trees := .I]
#Plot using ggplot 2
ggplot(oobdata, aes(x = trees, y= error, color = variable)) + geom_line()
#Plot using ggplot 2
ggplot(data = oobdata2, aes(x = trees, y= error, color = variable)) + geom_line()
oobdata2 = melt(oobdata, id.vars = "trees")
setnames(oobdata2, "value", "error")
ggplot(data = oobdata2, aes(x = trees, y= error, color = variable)) + geom_line()
hist(treesize(fit_random))
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set_random <- Affairs[forTrainig == 1,]
test_set_random <- Affairs[forTrainig ==2, ]
nrow(train_set_random)
nrow(test_set_random)
class(train_set_random)
fit_random <- randomForest(affairs~. , data = train_set_random, ntree = 100)
summary(fit_random)
#GET OBB data from the plot and coerce to data.table
library(data.table)
oobdata = as.data.table(plot(fit_random))
#cast to long format
oobdata2 = melt(oobdata, id.vars = "trees")
setnames(oobdata2, "value", "error")
#Plot using ggplot 2
ggplot(data = oobdata2, aes(x = trees, y= error, color = variable)) + geom_line()
#Define trees as 1:ntree
oobdata[,trees := .I]
predic_random <- predict(fit_random, newdata = test_set_random, type = "class")
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
hist(treesize(fit_random))
varImp(fit_random)
varImpPlot(fit_random, sort = TRUE, ttype = 2)
importance(fit_random)
varUsed(fit_random)
model <- randomForest(train_set_random$affairs~., data = train_set_random, ntree = 100, mtry = i, importance = TRUE)
a <- c()
i <-  5
for(i and 3:8){
model3 <- randomForest(train_set_random$affairs~., data = train_set_random, ntree = 100, mtry = i, importance = TRUE)
predValid <- predict(model3, test_set_random, type = "class")
a[i-2] =mean(predValid == test_set_random$affairs)
}
a <- c()
i <-  5
for(i in 3:8){
model3 <- randomForest(train_set_random$affairs~., data = train_set_random, ntree = 100, mtry = i, importance = TRUE)
predValid <- predict(model3, test_set_random, type = "class")
a[i-2] =mean(predValid == test_set_random$affairs)
}
a
plot(3:8,a)
head(Affairs, 20)
confusionMatrix(table(predic_random,test_set_random$affairs))
#Random forest
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set_random <- Affairs[forTrainig == 1,]
test_set_random <- Affairs[forTrainig ==2, ]
nrow(train_set_random)
nrow(test_set_random)
class(train_set_random)
fit_random <- randomForest(affairs~. , data = train_set_random, ntree = 100, mtry = 3)
summary(fit_random)
#OBB DATA______________________________________________________________________________________
#GET OBB data from the plot and coerce to data.table
library(data.table)
oobdata = as.data.table(plot(fit_random))
#cast to long format
oobdata2 = melt(oobdata, id.vars = "trees")
setnames(oobdata2, "value", "error")
#Plot using ggplot 2
ggplot(data = oobdata2, aes(x = trees, y= error, color = variable)) + geom_line()
#Define trees as 1:ntree
oobdata[,trees := .I]
predic_random <- predict(fit_random, newdata = test_set_random, type = "class")
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
library(randomForest)
set.seed(10)
forTrainig <- sample(2, nrow(Affairs), prob = c(0.7,0.3), replace = TRUE)
train_set_random <- Affairs[forTrainig == 1,]
test_set_random <- Affairs[forTrainig ==2, ]
nrow(train_set_random)
nrow(test_set_random)
class(train_set_random)
fit_random <- randomForest(affairs~. , data = train_set_random, ntree = 100, mtry = 3, importance = TRUE)
summary(fit_random)
#OBB DATA______________________________________________________________________________________
#GET OBB data from the plot and coerce to data.table
library(data.table)
oobdata = as.data.table(plot(fit_random))
#cast to long format
oobdata2 = melt(oobdata, id.vars = "trees")
setnames(oobdata2, "value", "error")
#Plot using ggplot 2
ggplot(data = oobdata2, aes(x = trees, y= error, color = variable)) + geom_line()
#Define trees as 1:ntree
oobdata[,trees := .I]
predic_random <- predict(fit_random, newdata = test_set_random, type = "class")
summary(predic_random)
confusionMatrix(table(predic_random,test_set_random$affairs))
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
diabetic<-read.csv("Diabetes.csv")
summary(diabetic)
str(diabetic)
diabetic$Is_Diabetic<-as.factor(diabetic$Is_Diabetic)
#cdplot(diabetic$Is_Diabetic~diabetic$BMI, data=diabetic)
#Build a training and testing set
set.seed(4)
id<-sample(2,nrow(diabetic),prob = c(0.7,0.3),replace = T)
trainset<-diabetic[id==1,]
testset<-diabetic[id==2,]
class(trainset)
#Build logistic regression model
model<-glm(trainset$Is_Diabetic~. ,data = trainset,family = "binomial")
summary(model)
diabetic<-read.csv(C:/Users/ICH/Google Drive/Data Science/Edureka/Data Science Certification with R/338_m5_dataset_v3.0/"Diabetes.csv")
diabetic<-read.csv(C:/Users/ICH/Google Drive/Data Science/Edureka/Data Science Certification with R/338_m5_dataset_v3.0"Diabetes.csv")
diabetic<-read.csv(C:/Users/ICH/Google Drive/Data Science/Edureka/Data Science Certification with R/338_m5_dataset_v3.0/Diabetes.csv)
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
setwd(C:\Users\ICH\Google Drive\Data Science\Edureka\Data Science Certification with R\338_m5_dataset_v3.0)
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
setwd(C:/Users/ICH/Google Drive/Data Science/Edureka/Data Science Certification with R/338_m5_dataset_v3.0)
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
setwd(C:\Users\ICH\Google Drive\Data Science\Edureka\Data Science Certification with R\338_m5_dataset_v3.0)
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
setwd("C:\Users\ICH\Google Drive\Data Science\Edureka\Data Science Certification with R\338_m5_dataset_v3.0")
# Y can take values between 0 and 1 whereas betax can take values between -infi to +infi
# Hence logit function becomes - log(p/1-p) = beta0 + beta1*x + error - log odds
#if x changes by one unit how odds changes
#It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).
setwd("C:/Users/ICH/Google Drive/Data Science/Edureka/Data Science Certification with R/338_m5_dataset_v3.0")
diabetic<-read.csv("Diabetes.csv")
summary(diabetic)
str(diabetic)
diabetic$Is_Diabetic<-as.factor(diabetic$Is_Diabetic)
#cdplot(diabetic$Is_Diabetic~diabetic$BMI, data=diabetic)
cdplot(diabetic$Is_Diabetic~diabetic$BMI, data=diabetic)
set.seed(4)
id<-sample(2,nrow(diabetic),prob = c(0.7,0.3),replace = T)
trainset<-diabetic[id==1,]
testset<-diabetic[id==2,]
class(trainset)
model<-glm(trainset$Is_Diabetic~. ,data = trainset,family = "binomial")
summary(model)
exp(0.085879)
(exp(0.085879)-1)*100
(exp(-0.004797)-1)*100
predvalues<-predict(model,newdata = testset,type = "response")
predvalues
table(Actualvalues = testset$Is_Diabetic, Predictedvalues = predvalues>0.5)
Accuracy <- (132+47)/(132+20+35+47)
# Accuracy is 76.4%
table(Actualvalues = testset$Is_Diabetic, Predictedvalues = predvalues>0.3)
Accuracy <- (111+64)/(111+64+41+18)
# Accuracy is 74.7%
table(Actualvalues = testset$Is_Diabetic, Predictedvalues = predvalues>0.7)
Accuracy <- (141+33)/(141+33+11+49)
# Accuracy is 74.3%
install.packages("ROCR")
library(ROCR)
ROCpred <-prediction(predvalues, testset$Is_Diabetic)
ROCperf <- performance(ROCpred, "tpr", "fpr")
plot(ROCperf, col = "blue", print.cutoffs.at = seq(0.1, by = 0.1),text.adj = c(0.2,1.7), cex = 0.7, colorize=TRUE)
table(Actualvalues = testset$Is_Diabetic, Predictedvalues = predvalues>0.4)
Accuracy <- (121+52)/(121+52+31+30)
diabetic<-read.csv("Diabetes.csv")
summary(diabetic)
diabetic$Is_Diabetic<-as.factor(diabetic$Is_Diabetic)
install.packages("rpart")
install.packages("rpart")
library(rpart)
install.packages("rpart")
install.packages("rpart")
#Build a training and testing set
set.seed(4)
id<-sample(2,nrow(diabetic),prob = c(0.7,0.3),replace = T)
trainset<-diabetic[id==1,]
testset<-diabetic[id==2,]
class(trainset)
model<-rpart(trainset$Is_Diabetic~. ,data = trainset)
library(rpart)
model<-rpart(trainset$Is_Diabetic~. ,data = trainset)
summary(model)
plot(model, margin=0.1)
plot(model, margin=0.1)
text(model, use.n = TRUE,pretty = TRUE, cex=0.8)
temp <- trainset[trainset$glucose_conc<154.5 & trainset$BMI<26.35,]
table(temp$Is_Diabetic)
predvalues<-predict(model,newdata = testset,type = "class")
predvalues
class(predvalues)
diabetic$predvalues <- predvalues
library(caret)
library(e1071)
confusionMatrix(table(predvalues, testset$Is_Diabetic))
diabetic<-read.csv("Diabetes.csv")
summary(diabetic)
diabetic$Is_Diabetic<-as.factor(diabetic$Is_Diabetic)
library(randomForest)
#Build a training and testing set
set.seed(4)
id<-sample(2,nrow(diabetic),prob = c(0.7,0.3),replace = T)
trainset<-diabetic[id==1,]
testset<-diabetic[id==2,]
class(trainset)
#Build random forest model
model<-randomForest(trainset$Is_Diabetic~. ,data = trainset, ntree=200)
model
plot(model)
library(data.table)
library(ggplot2)
# Get OOB data from plot and coerce to data.table
oobData = as.data.table(plot(model))
# Define trees as 1:ntree
oobData[, trees := .I]
# Cast to long format
oobData2 = melt(oobData, id.vars = "trees")
setnames(oobData2, "value", "error")
# Plot using ggplot
ggplot(data = oobData2, aes(x = trees, y = error, color = variable)) + geom_line()
predvalues<-predict(model,newdata = testset,type = "class")
predvalues
confusionMatrix(table(predvalues, testset$Is_Diabetic))
hist(treesize(model))
varImp(model)
varImpPlot(model,sort=TRUE, type=2)
importance(model)
varUsed(model)
a=c()
i=5
for (i in 3:8) {
model3 <- randomForest(trainset$Is_Diabetic~. ,data = trainset, ntree = 500, mtry = i, importance = TRUE)
predValid <- predict(model3, testset, type = "class")
a[i-2] = mean(predValid == testset$Is_Diabetic)
}
a
plot(3:8,a)
